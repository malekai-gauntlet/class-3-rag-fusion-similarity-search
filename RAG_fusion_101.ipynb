{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80DlzNJt0M5r"
      },
      "source": [
        "# RAG Fusion\n",
        "\n",
        "Re-implemented from [this GitHub repo](https://github.com/Raudaschl/rag-fusion), all credit to the original author."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opAjXPLKzzQL"
      },
      "source": [
        "## Setup\n",
        "\n",
        "For this example, we will use Pinecone and some fake data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFJ_d8yfv9ig"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
        "index_name = os.getenv(\"PINECONE_INDEX_TWO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhaU7_tq0ZS8"
      },
      "source": [
        "#### Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f0dEqFufw4PS",
        "outputId": "30cc65ff-c855-4af1-f569-8d5e8e6ab3c9"
      },
      "outputs": [],
      "source": [
        "# %pip install langchain_openai langchain_pinecone langchain_core langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2r_Fo5HxQiG"
      },
      "source": [
        "#### Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvGIa-Bnw3zj"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa-Wti_3xMxx"
      },
      "source": [
        "#### Load Fake Data\n",
        "\n",
        "Here we define a set of fake documents related to climate change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djfCk_qpxMMP"
      },
      "outputs": [],
      "source": [
        "all_documents = {\n",
        "    \"doc1\": \"Climate change and economic impact.\",\n",
        "    \"doc2\": \"Public health concerns due to climate change.\",\n",
        "    \"doc3\": \"Climate change: A social perspective.\",\n",
        "    \"doc4\": \"Technological solutions to climate change.\",\n",
        "    \"doc5\": \"Policy changes needed to combat climate change.\",\n",
        "    \"doc6\": \"Climate change and its impact on biodiversity.\",\n",
        "    \"doc7\": \"Climate change: The science and models.\",\n",
        "    \"doc8\": \"Global warming: A subset of climate change.\",\n",
        "    \"doc9\": \"How climate change affects daily weather.\",\n",
        "    \"doc10\": \"The history of climate change activism.\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_cqV8faxZca"
      },
      "source": [
        "#### Create Vector Store\n",
        "\n",
        "We will use Pinecone to create a vector store from these documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFK8ViaOxT5Q"
      },
      "outputs": [],
      "source": [
        "vectorstore = PineconeVectorStore.from_texts(\n",
        "    list(all_documents.values()), OpenAIEmbeddings(), index_name=index_name\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjU_T7V-043M"
      },
      "source": [
        "## Define the Query Generator\n",
        "\n",
        "We will now define a chain to do the query generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBmd8bte1KXL"
      },
      "source": [
        "#### Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz77xXjixq3t"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WlOcFXBWxzJN",
        "outputId": "cd5e93c2-3ef8-4474-f1c0-5a94cacabf6a"
      },
      "outputs": [],
      "source": [
        "# %pip install langchainhub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpZpUlRq1PLK"
      },
      "source": [
        "#### Load Prompt From LangChainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdA47rbexuLf"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"langchain-ai/rag-fusion-query-generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYOwGJmJx6Iw"
      },
      "outputs": [],
      "source": [
        "# from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# prompt = ChatPromptTemplate.from_messages([\n",
        "#     (\"system\", \"You are a helpful assistant that generates multiple search queries based on a single input query.\"),\n",
        "#     (\"user\", \"Generate multiple search queries related to: {original_query}\"),\n",
        "#     (\"user\", \"OUTPUT (4 queries):\")\n",
        "# ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9IZQDMQ1hFp"
      },
      "source": [
        "#### Define Query Generation Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0EDAifOx8w7"
      },
      "outputs": [],
      "source": [
        "generate_queries = (\n",
        "    prompt | ChatOpenAI(temperature=0) | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi7-FQji1ndQ"
      },
      "source": [
        "## Define the Full Chain\n",
        "\n",
        "We can now put it all together and define the full chain. This chain:\n",
        "\n",
        "1. Generates a bunch of queries\n",
        "2. Looks up each query in the retriever\n",
        "3. Joins all the results together using reciprocal rank fusion\n",
        "\n",
        "Note that it does NOT do a final generation step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiZY3qbB1xh1"
      },
      "source": [
        "#### Original Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kl_fDxEoyAT9"
      },
      "outputs": [],
      "source": [
        "original_query = \"impact of climate change\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa8ebg9r1wkG"
      },
      "source": [
        "#### Create Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqd-prFvyDLR"
      },
      "outputs": [],
      "source": [
        "vectorstore = PineconeVectorStore.from_existing_index(index_name, OpenAIEmbeddings())\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBiQvKTq16JO"
      },
      "source": [
        "#### Define Reciprocal Rank Fusion Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sqkoL6ayH6I"
      },
      "outputs": [],
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    fused_scores = {}\n",
        "    for docs in results:\n",
        "        # Assumes the docs are returned in sorted order of relevance\n",
        "        for rank, doc in enumerate(docs):\n",
        "            doc_str = dumps(doc)\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "    return reranked_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pFpmIHj19-Y"
      },
      "source": [
        "#### Define Full Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj-2Myd7yLaJ"
      },
      "outputs": [],
      "source": [
        "chain = generate_queries | retriever.map() | reciprocal_rank_fusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPiYkR9K2BGx"
      },
      "source": [
        "#### Invoke Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M3Bk79ZyQ7E",
        "outputId": "e78b3908-b27a-418b-b003-f2cb185792bd"
      },
      "outputs": [],
      "source": [
        "chain.invoke({\"original_query\": original_query})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
